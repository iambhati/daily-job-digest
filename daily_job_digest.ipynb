{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCew03wVSP/wffoTn05aMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iambhati/daily-job-digest/blob/main/daily_job_digest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsJEGQOK1zXd",
        "outputId": "37c9038d-5efd-4327-9589-cc4c0aff5dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.4)\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 schedule\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG_MODE = True  # Keep this True initially"
      ],
      "metadata": {
        "id": "b0BZ2Nz7Yz3q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install webdriver-manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC5l1pksYxQc",
        "outputId": "8a92aa7a-4007-41ed-f0c4-09b86a647e19"
      },
      "execution_count": 26,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.12/dist-packages (4.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (2.32.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium webdriver-manager requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taToAUSSYSJC",
        "outputId": "6c90b7ab-6ab9-448d-ce17-4e115350d731"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.4)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enhanced Daily Job Digest Script with Better Location Filtering and Remote Jobs\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import time\n",
        "import random\n",
        "from urllib.parse import quote_plus\n",
        "import re\n",
        "\n",
        "# ---------------- USER SETTINGS ----------------\n",
        "KEYWORDS = [\n",
        "    \"Entry level Data Analyst\",\n",
        "    \"Entry level Business Analyst\",\n",
        "    \"Junior Analyst\",\n",
        "    \"Fresher Analyst\",\n",
        "    \"Associate Data Analyst\",\n",
        "    \"Associate Business Analyst\",\n",
        "    \"Trainee Analyst\",\n",
        "    \"Graduate Analyst\",\n",
        "    \"Junior Data Scientist\",\n",
        "    \"Business Intelligence Analyst\"\n",
        "]\n",
        "\n",
        "# Target locations - expanded list\n",
        "TARGET_LOCATIONS = [\"Gurgaon\", \"Noida\", \"Jaipur\", \"Delhi\", \"NCR\", \"Gurugram\", \"New Delhi\"]\n",
        "INCLUDE_REMOTE = True\n",
        "\n",
        "# Remote work keywords - comprehensive list\n",
        "REMOTE_KEYWORDS = [\n",
        "    \"remote\", \"work from home\", \"wfh\", \"hybrid\", \"anywhere\", \"telecommute\",\n",
        "    \"distributed\", \"virtual\", \"home office\", \"flexible location\", \"remote work\",\n",
        "    \"work remotely\", \"home based\", \"location independent\"\n",
        "]\n",
        "\n",
        "RESULTS_PER_SITE = 10  # Increased from 5\n",
        "SENDER_EMAIL = \"learnxaiml@gmail.com\"\n",
        "APP_PASSWORD = \"jkfr lftg gjta sadq\"\n",
        "RECEIVER_EMAIL = \"learnxaiml@gmail.com\"\n",
        "\n",
        "# ---------------- ENHANCED HELPERS ----------------\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def is_remote_job(location: str, title: str = \"\", description: str = \"\") -> bool:\n",
        "    \"\"\"Enhanced remote job detection\"\"\"\n",
        "    if not location and not title and not description:\n",
        "        return False\n",
        "\n",
        "    text_to_check = f\"{location} {title} {description}\".lower()\n",
        "    return any(keyword in text_to_check for keyword in REMOTE_KEYWORDS)\n",
        "\n",
        "def location_allowed(location: str, title: str = \"\", description: str = \"\") -> bool:\n",
        "    \"\"\"Enhanced location filtering with better remote detection\"\"\"\n",
        "    if not location:\n",
        "        return False\n",
        "\n",
        "    loc_lower = location.lower()\n",
        "\n",
        "    # Check for remote work\n",
        "    if INCLUDE_REMOTE and is_remote_job(location, title, description):\n",
        "        return True\n",
        "\n",
        "    # Check for target cities\n",
        "    for city in TARGET_LOCATIONS:\n",
        "        if city.lower() in loc_lower:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    if not text:\n",
        "        return \"N/A\"\n",
        "    return re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "def random_delay():\n",
        "    \"\"\"Add random delay to avoid being blocked\"\"\"\n",
        "    time.sleep(random.uniform(1, 3))\n",
        "\n",
        "# ---------------- ENHANCED SCRAPERS ----------------\n",
        "def search_naukri(keyword):\n",
        "    \"\"\"Enhanced Naukri scraper with multiple selectors\"\"\"\n",
        "    jobs = []\n",
        "    search_term = keyword.replace(' ', '-').lower()\n",
        "    url = f\"https://www.naukri.com/{search_term}-jobs\"\n",
        "\n",
        "    try:\n",
        "        random_delay()\n",
        "        r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        # Try multiple selectors for job cards\n",
        "        selectors = [\n",
        "            \"article.jobTuple\",\n",
        "            \"div.jobTuple\",\n",
        "            \"div[class*='jobTuple']\",\n",
        "            \"div.srp-jobtuple-wrapper\",\n",
        "            \"div.job-tuple\"\n",
        "        ]\n",
        "\n",
        "        cards = []\n",
        "        for selector in selectors:\n",
        "            cards = soup.select(selector)\n",
        "            if cards:\n",
        "                break\n",
        "\n",
        "        cards = cards[:RESULTS_PER_SITE]\n",
        "        print(f\"[DEBUG] Found {len(cards)} Naukri cards for '{keyword}'\")\n",
        "\n",
        "        for card in cards:\n",
        "            # Try multiple selectors for each field\n",
        "            title_selectors = [\"a.title\", \"h2 a\", \"h3 a\", \"a[class*='title']\", \".jobTupleHeader a\"]\n",
        "            company_selectors = [\"a.subTitle\", \".companyInfo a\", \"a[class*='subTitle']\", \".company a\"]\n",
        "            location_selectors = [\"li.location\", \".locWdth\", \".location\", \"[class*='location']\"]\n",
        "\n",
        "            title = company = location = link = None\n",
        "\n",
        "            # Extract title and link\n",
        "            for sel in title_selectors:\n",
        "                elem = card.select_one(sel)\n",
        "                if elem:\n",
        "                    title = clean_text(elem.get_text())\n",
        "                    link = elem.get('href', '#')\n",
        "                    if not link.startswith('http'):\n",
        "                        link = f\"https://www.naukri.com{link}\"\n",
        "                    break\n",
        "\n",
        "            # Extract company\n",
        "            for sel in company_selectors:\n",
        "                elem = card.select_one(sel)\n",
        "                if elem:\n",
        "                    company = clean_text(elem.get_text())\n",
        "                    break\n",
        "\n",
        "            # Extract location\n",
        "            for sel in location_selectors:\n",
        "                elem = card.select_one(sel)\n",
        "                if elem:\n",
        "                    location = clean_text(elem.get_text())\n",
        "                    break\n",
        "\n",
        "            if title and title != \"N/A\" and location_allowed(location, title):\n",
        "                jobs.append((title, company or \"N/A\", location or \"N/A\", link or \"#\"))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Naukri fetch failed for '{keyword}': {e}\")\n",
        "\n",
        "    return jobs\n",
        "\n",
        "def search_linkedin(keyword):\n",
        "    \"\"\"Enhanced LinkedIn scraper\"\"\"\n",
        "    jobs = []\n",
        "    search_term = quote_plus(keyword)\n",
        "    url = f\"https://www.linkedin.com/jobs/search?keywords={search_term}&location=India\"\n",
        "\n",
        "    try:\n",
        "        random_delay()\n",
        "        r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        # Try multiple selectors\n",
        "        selectors = [\n",
        "            \"div.base-card\",\n",
        "            \"li.result-card\",\n",
        "            \"div[class*='job-search-card']\",\n",
        "            \"div.job-card-container\"\n",
        "        ]\n",
        "\n",
        "        cards = []\n",
        "        for selector in selectors:\n",
        "            cards = soup.select(selector)\n",
        "            if cards:\n",
        "                break\n",
        "\n",
        "        cards = cards[:RESULTS_PER_SITE]\n",
        "        print(f\"[DEBUG] Found {len(cards)} LinkedIn cards for '{keyword}'\")\n",
        "\n",
        "        for card in cards:\n",
        "            # Multiple selectors for each field\n",
        "            title_selectors = [\"h3\", \"h3 a\", \".job-card-list__title\", \"a .sr-only\"]\n",
        "            company_selectors = [\"h4\", \"h4 a\", \".job-card-container__company-name\", \".job-result-card__company-name\"]\n",
        "            location_selectors = [\".job-search-card__location\", \".job-result-card__location\", \".job-card-container__metadata-item\"]\n",
        "            link_selectors = [\"a\", \"h3 a\"]\n",
        "\n",
        "            title = company = location = link = None\n",
        "\n",
        "            # Extract title\n",
        "            for sel in title_selectors:\n",
        "                elem = card.select_one(sel)\n",
        "                if elem:\n",
        "                    title = clean_text(elem.get_text())\n",
        "                    break\n",
        "\n",
        "            # Extract company\n",
        "            for sel in company_selectors:\n",
        "                elem = card.select_one(sel)\n",
        "                if elem:\n",
        "                    company = clean_text(elem.get_text())\n",
        "                    break\n",
        "\n",
        "            # Extract location\n",
        "            for sel in location_selectors:\n",
        "                elem = card.select_one(sel)\n",
        "                if elem:\n",
        "                    location = clean_text(elem.get_text())\n",
        "                    break\n",
        "\n",
        "            # Extract link\n",
        "            for sel in link_selectors:\n",
        "                elem = card.select_one(sel)\n",
        "                if elem and elem.get('href'):\n",
        "                    link = elem['href']\n",
        "                    if not link.startswith('http'):\n",
        "                        link = f\"https://www.linkedin.com{link}\"\n",
        "                    break\n",
        "\n",
        "            if title and title != \"N/A\" and location_allowed(location, title):\n",
        "                jobs.append((title, company or \"N/A\", location or \"N/A\", link or \"#\"))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] LinkedIn fetch failed for '{keyword}': {e}\")\n",
        "\n",
        "    return jobs\n",
        "\n",
        "def search_indeed(keyword):\n",
        "    \"\"\"New Indeed scraper\"\"\"\n",
        "    jobs = []\n",
        "    search_term = quote_plus(keyword)\n",
        "    url = f\"https://in.indeed.com/jobs?q={search_term}&l=India\"\n",
        "\n",
        "    try:\n",
        "        random_delay()\n",
        "        r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        cards = soup.select(\"div.job_seen_beacon, div[data-jk], td.resultContent\")[:RESULTS_PER_SITE]\n",
        "        print(f\"[DEBUG] Found {len(cards)} Indeed cards for '{keyword}'\")\n",
        "\n",
        "        for card in cards:\n",
        "            title_elem = card.select_one(\"h2 a span, h2 span a, a[data-testid='job-title']\")\n",
        "            company_elem = card.select_one(\"span.companyName, a .companyName, span[data-testid='company-name']\")\n",
        "            location_elem = card.select_one(\"div.companyLocation, div[data-testid='job-location']\")\n",
        "            link_elem = card.select_one(\"h2 a, a[data-testid='job-title']\")\n",
        "\n",
        "            title = clean_text(title_elem.get_text()) if title_elem else \"N/A\"\n",
        "            company = clean_text(company_elem.get_text()) if company_elem else \"N/A\"\n",
        "            location = clean_text(location_elem.get_text()) if location_elem else \"N/A\"\n",
        "            link = link_elem.get('href', '#') if link_elem else \"#\"\n",
        "\n",
        "            if not link.startswith('http') and link != \"#\":\n",
        "                link = f\"https://in.indeed.com{link}\"\n",
        "\n",
        "            if title != \"N/A\" and location_allowed(location, title):\n",
        "                jobs.append((title, company, location, link))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Indeed fetch failed for '{keyword}': {e}\")\n",
        "\n",
        "    return jobs\n",
        "\n",
        "def search_glassdoor(keyword):\n",
        "    \"\"\"New Glassdoor scraper\"\"\"\n",
        "    jobs = []\n",
        "    search_term = quote_plus(keyword)\n",
        "    url = f\"https://www.glassdoor.co.in/Job/jobs.htm?sc.keyword={search_term}&locT=N&locId=115\"\n",
        "\n",
        "    try:\n",
        "        random_delay()\n",
        "        r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        cards = soup.select(\"li[data-test='jobListing'], div.react-job-listing\")[:RESULTS_PER_SITE]\n",
        "        print(f\"[DEBUG] Found {len(cards)} Glassdoor cards for '{keyword}'\")\n",
        "\n",
        "        for card in cards:\n",
        "            title_elem = card.select_one(\"a[data-test='job-link'], .jobLink\")\n",
        "            company_elem = card.select_one(\"[data-test='employer-name'], .jobEmpolyerName\")\n",
        "            location_elem = card.select_one(\"[data-test='job-location'], .jobLocation\")\n",
        "\n",
        "            title = clean_text(title_elem.get_text()) if title_elem else \"N/A\"\n",
        "            company = clean_text(company_elem.get_text()) if company_elem else \"N/A\"\n",
        "            location = clean_text(location_elem.get_text()) if location_elem else \"N/A\"\n",
        "            link = title_elem.get('href', '#') if title_elem else \"#\"\n",
        "\n",
        "            if not link.startswith('http') and link != \"#\":\n",
        "                link = f\"https://www.glassdoor.co.in{link}\"\n",
        "\n",
        "            if title != \"N/A\" and location_allowed(location, title):\n",
        "                jobs.append((title, company, location, link))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Glassdoor fetch failed for '{keyword}': {e}\")\n",
        "\n",
        "    return jobs\n",
        "\n",
        "# ---------------- ENHANCED EMAIL ----------------\n",
        "def send_email(jobs):\n",
        "    \"\"\"Enhanced email with better formatting\"\"\"\n",
        "    now = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    subject = f\"🎯 Daily Analyst Job Digest – {now} ({len(jobs)} opportunities)\"\n",
        "\n",
        "    # Separate remote and location-based jobs\n",
        "    remote_jobs = []\n",
        "    location_jobs = []\n",
        "\n",
        "    for job in jobs:\n",
        "        title, company, location, link = job\n",
        "        if is_remote_job(location, title):\n",
        "            remote_jobs.append(job)\n",
        "        else:\n",
        "            location_jobs.append(job)\n",
        "\n",
        "    body = f\"\"\"\n",
        "    <html>\n",
        "    <body style=\"font-family: Arial, sans-serif; line-height: 1.6; color: #333;\">\n",
        "        <h2 style=\"color: #2c5aa0;\">🎯 Daily Analyst Job Opportunities</h2>\n",
        "        <p>Hello Sam,</p>\n",
        "        <p>Here are today's top entry-level analyst job opportunities ({len(jobs)} total jobs found):</p>\n",
        "    \"\"\"\n",
        "\n",
        "    if remote_jobs:\n",
        "        body += f\"\"\"\n",
        "        <h3 style=\"color: #28a745;\">🏠 Remote/WFH Opportunities ({len(remote_jobs)} jobs)</h3>\n",
        "        <div style=\"margin-left: 20px;\">\n",
        "        \"\"\"\n",
        "        for job in remote_jobs:\n",
        "            title, company, location, link = job\n",
        "            body += f\"\"\"\n",
        "            <div style=\"margin-bottom: 15px; padding: 10px; border-left: 3px solid #28a745; background-color: #f8f9fa;\">\n",
        "                <strong style=\"color: #28a745;\">{title}</strong><br>\n",
        "                <span style=\"color: #666;\">{company} – {location}</span><br>\n",
        "                <a href=\"{link}\" style=\"color: #2c5aa0; text-decoration: none;\">Apply Here →</a>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        body += \"</div>\"\n",
        "\n",
        "    if location_jobs:\n",
        "        body += f\"\"\"\n",
        "        <h3 style=\"color: #dc3545;\">📍 Location-Based Opportunities ({len(location_jobs)} jobs)</h3>\n",
        "        <div style=\"margin-left: 20px;\">\n",
        "        \"\"\"\n",
        "        for job in location_jobs:\n",
        "            title, company, location, link = job\n",
        "            body += f\"\"\"\n",
        "            <div style=\"margin-bottom: 15px; padding: 10px; border-left: 3px solid #dc3545; background-color: #f8f9fa;\">\n",
        "                <strong style=\"color: #dc3545;\">{title}</strong><br>\n",
        "                <span style=\"color: #666;\">{company} – {location}</span><br>\n",
        "                <a href=\"{link}\" style=\"color: #2c5aa0; text-decoration: none;\">Apply Here →</a>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        body += \"</div>\"\n",
        "\n",
        "    body += \"\"\"\n",
        "        <hr style=\"margin: 30px 0;\">\n",
        "        <p style=\"color: #666; font-size: 14px;\">\n",
        "            💡 <strong>Tips:</strong><br>\n",
        "            • Apply early for better chances<br>\n",
        "            • Customize your resume for each role<br>\n",
        "            • Research the company before applying<br>\n",
        "            • Follow up after 1-2 weeks if no response\n",
        "        </p>\n",
        "        <p>Good luck with your applications! 🚀</p>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    msg = MIMEMultipart()\n",
        "    msg[\"From\"] = SENDER_EMAIL\n",
        "    msg[\"To\"] = RECEIVER_EMAIL\n",
        "    msg[\"Subject\"] = subject\n",
        "    msg.attach(MIMEText(body, \"html\"))\n",
        "\n",
        "    try:\n",
        "        server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
        "        server.starttls()\n",
        "        server.login(SENDER_EMAIL, APP_PASSWORD)\n",
        "        server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())\n",
        "        server.quit()\n",
        "        print(f\"[INFO] ✅ Email sent successfully to {RECEIVER_EMAIL}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ❌ Failed to send email: {e}\")\n",
        "\n",
        "# ---------------- ENHANCED MAIN ----------------\n",
        "def fetch_all_and_send():\n",
        "    \"\"\"Enhanced main function with multiple job sites\"\"\"\n",
        "    print(f\"[INFO] 🔍 Starting job search at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"[INFO] 📍 Target locations: {', '.join(TARGET_LOCATIONS)}\")\n",
        "    print(f\"[INFO] 🏠 Include remote: {INCLUDE_REMOTE}\")\n",
        "\n",
        "    all_jobs = []\n",
        "\n",
        "    # Search functions with their names\n",
        "    search_functions = [\n",
        "        (\"Naukri\", search_naukri),\n",
        "        (\"LinkedIn\", search_linkedin),\n",
        "        (\"Indeed\", search_indeed),\n",
        "        (\"Glassdoor\", search_glassdoor)\n",
        "    ]\n",
        "\n",
        "    for kw in KEYWORDS:\n",
        "        print(f\"\\n[INFO] 🔎 Searching for: '{kw}'\")\n",
        "        for site_name, search_func in search_functions:\n",
        "            try:\n",
        "                jobs = search_func(kw)\n",
        "                all_jobs.extend(jobs)\n",
        "                print(f\"[INFO] {site_name}: Found {len(jobs)} jobs\")\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] {site_name}: Search failed - {e}\")\n",
        "\n",
        "    print(f\"\\n[INFO] 📊 Total jobs before deduplication: {len(all_jobs)}\")\n",
        "\n",
        "    # Enhanced deduplication - normalize titles for better matching\n",
        "    def normalize_title(title):\n",
        "        return re.sub(r'[^\\w\\s]', '', title.lower()).strip()\n",
        "\n",
        "    seen = set()\n",
        "    unique_jobs = []\n",
        "    for job in all_jobs:\n",
        "        title, company, location, link = job\n",
        "        # Create key with normalized title and company\n",
        "        key = (normalize_title(title), company.lower().strip())\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique_jobs.append(job)\n",
        "\n",
        "    print(f\"[INFO] 📊 Unique jobs after deduplication: {len(unique_jobs)}\")\n",
        "\n",
        "    if unique_jobs:\n",
        "        # Sort jobs - remote jobs first, then by company name\n",
        "        unique_jobs.sort(key=lambda x: (not is_remote_job(x[2], x[0]), x[1].lower()))\n",
        "        send_email(unique_jobs)\n",
        "\n",
        "        # Print summary\n",
        "        remote_count = sum(1 for job in unique_jobs if is_remote_job(job[2], job[0]))\n",
        "        location_count = len(unique_jobs) - remote_count\n",
        "        print(f\"[INFO] 📧 Email sent with {len(unique_jobs)} jobs ({remote_count} remote, {location_count} location-based)\")\n",
        "    else:\n",
        "        print(\"[INFO] ❌ No matching jobs found today.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fetch_all_and_send()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkTYiMC62EH2",
        "outputId": "3c16c83f-420e-48b1-dd44-4d4c337b52f6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 🔍 Starting job search at 2025-08-25 18:18:10\n",
            "[INFO] 📍 Target locations: Gurgaon, Noida, Jaipur, Delhi, NCR, Gurugram, New Delhi\n",
            "[INFO] 🏠 Include remote: True\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Entry level Data Analyst'\n",
            "[DEBUG] Found 0 Naukri cards for 'Entry level Data Analyst'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Entry level Data Analyst'\n",
            "[INFO] LinkedIn: Found 0 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Entry level Data Analyst'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Entry level Data Analyst'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Entry level Business Analyst'\n",
            "[DEBUG] Found 0 Naukri cards for 'Entry level Business Analyst'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Entry level Business Analyst'\n",
            "[INFO] LinkedIn: Found 2 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Entry level Business Analyst'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Entry level Business Analyst'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Junior Analyst'\n",
            "[DEBUG] Found 0 Naukri cards for 'Junior Analyst'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Junior Analyst'\n",
            "[INFO] LinkedIn: Found 2 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Junior Analyst'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Junior Analyst'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Fresher Analyst'\n",
            "[DEBUG] Found 0 Naukri cards for 'Fresher Analyst'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Fresher Analyst'\n",
            "[INFO] LinkedIn: Found 7 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Fresher Analyst'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Fresher Analyst'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Associate Data Analyst'\n",
            "[DEBUG] Found 0 Naukri cards for 'Associate Data Analyst'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Associate Data Analyst'\n",
            "[INFO] LinkedIn: Found 1 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Associate Data Analyst'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Associate Data Analyst'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Associate Business Analyst'\n",
            "[DEBUG] Found 0 Naukri cards for 'Associate Business Analyst'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Associate Business Analyst'\n",
            "[INFO] LinkedIn: Found 4 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Associate Business Analyst'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Associate Business Analyst'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Trainee Analyst'\n",
            "[DEBUG] Found 0 Naukri cards for 'Trainee Analyst'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Trainee Analyst'\n",
            "[INFO] LinkedIn: Found 7 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Trainee Analyst'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Trainee Analyst'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Graduate Analyst'\n",
            "[DEBUG] Found 0 Naukri cards for 'Graduate Analyst'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Graduate Analyst'\n",
            "[INFO] LinkedIn: Found 4 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Graduate Analyst'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Graduate Analyst'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Junior Data Scientist'\n",
            "[DEBUG] Found 0 Naukri cards for 'Junior Data Scientist'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Junior Data Scientist'\n",
            "[INFO] LinkedIn: Found 0 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Junior Data Scientist'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Junior Data Scientist'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 🔎 Searching for: 'Business Intelligence Analyst'\n",
            "[DEBUG] Found 0 Naukri cards for 'Business Intelligence Analyst'\n",
            "[INFO] Naukri: Found 0 jobs\n",
            "[DEBUG] Found 10 LinkedIn cards for 'Business Intelligence Analyst'\n",
            "[INFO] LinkedIn: Found 0 jobs\n",
            "[DEBUG] Found 0 Indeed cards for 'Business Intelligence Analyst'\n",
            "[INFO] Indeed: Found 0 jobs\n",
            "[DEBUG] Found 0 Glassdoor cards for 'Business Intelligence Analyst'\n",
            "[INFO] Glassdoor: Found 0 jobs\n",
            "\n",
            "[INFO] 📊 Total jobs before deduplication: 27\n",
            "[INFO] 📊 Unique jobs after deduplication: 14\n",
            "[INFO] ✅ Email sent successfully to learnxaiml@gmail.com\n",
            "[INFO] 📧 Email sent with 14 jobs (1 remote, 13 location-based)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vPtlEgEY4QLd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}